[{"content":"\u003cp\u003eMy skills and experience chatbot - ask anything!\u003c/p\u003e\n\n\n\u003ciframe\n  src=\"https://huggingface.co/spaces/heleh/hele-experience-chatbot\"\n  width=\"100%\"\n  height=\"600\"\n  frameborder=\"0\"\n  allow=\"clipboard-write; microphone\"\n  allowfullscreen\u003e\n\u003c/iframe\u003e","description":null,"image":null,"permalink":"http://localhost:1313/blogs/07_helena_chatbot/","title":"Chatbot: Helena's resume"},{"content":"\u003cp\u003eHere we will go through a supermarket transaction data to understand better our customer segments and their purchasing behaviors. The data and task is from a data analytics virtual work experience program I completed with \u003ca href=\"https://www.theforage.com/virtual-internships/prototype/NkaC7knWtjSbi6aYv/Data-Analytics?ref=kh22opepLRepWYSK3\"\u003eForage\u003c/a\u003e and Quantium, a leading data science and AI Firm with a track record of innovation in data science.\u003c/p\u003e\n\u003cp\u003eCode for this project is available on my \u003ca href=\"https://github.com/HHook/QUANTIUM-virtual-internship\"\u003egithub\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"overview\"\u003eOverview\u003c/h1\u003e\n\u003cp\u003eThese tasks work through a common narrative, designed to replicate life in the Retail Analytics and Strategy team at Quantium.\u003c/p\u003e\n\u003cp\u003eQuantium has had a data partnership with a large supermarket brand for the last few years who provide transactional and customer data. We are goung through this problem as an analyst within the Quantium analytics team and are responsible for delivering highly valued data analytics and insights to help the business make strategic decisions.\u003c/p\u003e\n\u003ch2 id=\"the-task\"\u003eThe task\u003c/h2\u003e\n\u003cp\u003eWe need to analyse the data to understand current purchasing trends and behaviours. The client is particularly interested in customer segments and their chip purchasing behaviour.\u003c/p\u003e\n\u003cp\u003eOur insights need to have a commercial application as our end goal is to form a strategy based on the findings to provide a clear recommendation to the Category Manager.\u003c/p\u003e\n\u003ch1 id=\"pre-processing\"\u003ePre-processing\u003c/h1\u003e\n\u003cp\u003eWe will start with pre-processing our data before we get into analysing customer segments and sales.\u003c/p\u003e\n\u003cp\u003eHigh level data checks:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esummaries of the data\u003c/li\u003e\n\u003cli\u003efind outliers and removing these\u003c/li\u003e\n\u003cli\u003eCheck data formats and correct\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDerive extra features:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epack size\u003c/li\u003e\n\u003cli\u003ebrand name\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExamine:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003einconsistencies\u003c/li\u003e\n\u003cli\u003emissing data\u003c/li\u003e\n\u003cli\u003ecorrectly identified category items\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"explore-date\"\u003eExplore date\u003c/h1\u003e\n\u003cp\u003eStraight away we see that date is in a strange format. This is an old excel date format of days after 30/12/1899. We will convert it to a regular date.\u003c/p\u003e\n\u003c!-- *info https://support.smartbear.com/testcomplete/docs/scripting/working-with/dates/python.html*   --\u003e\n\u003cp\u003eFirst day: 2018-07-01 00:00:00\u003cbr\u003e\nLast day: 2019-06-30 00:00:00\u003cbr\u003e\nNumber of unique dates: 364\u003c/p\u003e\n\u003cp\u003eLooks like transactions from one day are missing (as there are usually 365 days per year and we found records for only 364 days.)  The missing date is Christmas day December 25th 2018, when the shop is closed so it is not an anomaly.\u003c/p\u003e\n\u003ch3 id=\"number-of-transactions-over-time\"\u003eNumber of transactions over time\u003c/h3\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/output_5_0.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/output_5_0.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\n\u003ch3 id=\"closer-look-at-december\"\u003eCloser look at December\u003c/h3\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/output_7_0.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/output_7_0.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\n\u003ch1 id=\"derive-features\"\u003eDerive features\u003c/h1\u003e\n\u003ch3 id=\"weight\"\u003eWeight\u003c/h3\u003e\n\u003cp\u003eBecause the weight of the product is attached to it\u0026rsquo;s product name, we will need to extract the weight from each product name to a new column.\u003c/p\u003e\n\n\n\u003cdiv\u003e\n\u003cstyle scoped\u003e\n\t.dataframe {font-size: 10pt;}\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: right;\n        padding: 1px;\n        padding-left:8px;\n        padding-right:8px;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: right;\n        padding: 1px;\n        padding-right:0px;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n        padding: 0px;\n        padding-left:5px;\n        padding-right:0px;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: left;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eProduct name\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e97\u003c/th\u003e\n      \u003ctd\u003eCheetos Puffs 165g\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e5\u003c/th\u003e\n      \u003ctd\u003eKettle 135g Swt Pot Sea Salt\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e57\u003c/th\u003e\n      \u003ctd\u003eBurger Rings 220g\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\u003cp\u003eMost popular pack sizes are 175g and 150g.\u003cbr\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/output_11_0.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/output_11_0.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/p\u003e\n\u003ch3 id=\"brand-name\"\u003eBrand name\u003c/h3\u003e\n\u003cp\u003eExtract brand name from product name as we did with the weight.\u003c/p\u003e\n\u003cp\u003eAfter checking the extracted brand names we will combine some that are the same but written in a different way.\u003cbr\u003e\n\u0026lsquo;Red\u0026rsquo; = \u0026lsquo;Red Rock Deli\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;RRD\u0026rsquo; = \u0026lsquo;Red Rock Deli\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;Old\u0026rsquo; = \u0026lsquo;Old El Paso\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;WW\u0026rsquo; = \u0026lsquo;Woolworths\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;Dorito\u0026rsquo; = \u0026lsquo;Doritos\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;Natural\u0026rsquo; = \u0026lsquo;Natural Chip Co\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;NCC\u0026rsquo; = \u0026lsquo;Natural Chip Co\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;Infzns\u0026rsquo; = \u0026lsquo;Infuzions\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;Snbts\u0026rsquo; = \u0026lsquo;Sunbites\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;GrnWves\u0026rsquo; = \u0026lsquo;Sunbites\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;Burger\u0026rsquo; = \u0026lsquo;Smiths\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;Smith\u0026rsquo; = \u0026lsquo;Smiths\u0026rsquo;\u003cbr\u003e\n\u0026lsquo;French\u0026rsquo; = \u0026lsquo;French Fries\u0026rsquo;\u003c/p\u003e\n\u003ch3 id=\"wrong-categories\"\u003eWrong categories\u003c/h3\u003e\n\u003cp\u003eLet\u0026rsquo;s find products that are not chips and remove them from our dataset.\u003c/p\u003e\n\u003cp\u003eOld El Paso Salsa Dips are wrongly categories under chips.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eOld El Paso Salsa   Dip Chnky Tom Ht300g    3125\nOld El Paso Salsa   Dip Tomato Med 300g     3114\nOld El Paso Salsa   Dip Tomato Mild 300g    3085\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWoolworth salsas wrongly categorised under chips.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eWoolworths Mild     Salsa 300g    1491\nWoolworths Medium   Salsa 300g    1430\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDoritos salsas wrongly categorised under chips.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eDoritos Salsa Mild  300g           1472\nDoritos Salsa       Medium 300g    1449\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDip in Smiths brand is just a name for chip flavor, not wrongly categorised so we will keep those.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSmiths Crinkle Cut  French OnionDip 150g    1438\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1 id=\"product-quantities\"\u003eProduct quantities\u003c/h1\u003e\n\u003cp\u003eAfter exploring in which quantities chips are bought we find an outlier on 2 records with 200 units bought each time. These to purchases are from the same customer and they don\u0026rsquo;t have any other records over the year. This could mean they are not a regular consumer and might be buying chips for commercial purposes. We will remove this customer\u0026rsquo;s records from our dataset.\u003c/p\u003e\n\n\n\u003cdiv\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: left;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eDATE\u003c/th\u003e\n      \u003cth\u003eSTORE_NBR\u003c/th\u003e\n      \u003cth\u003eLYLTY_CARD_NBR\u003c/th\u003e\n      \u003cth\u003eTXN_ID\u003c/th\u003e\n      \u003cth\u003ePROD_NBR\u003c/th\u003e\n      \u003cth\u003ePROD_NAME\u003c/th\u003e\n      \u003cth\u003ePROD_QTY\u003c/th\u003e\n      \u003cth\u003eTOT_SALES\u003c/th\u003e\n      \u003cth\u003eweight\u003c/th\u003e\n      \u003cth\u003ebrand\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e69762\u003c/th\u003e\n      \u003ctd\u003e2018-08-19\u003c/td\u003e\n      \u003ctd\u003e226\u003c/td\u003e\n      \u003ctd\u003e226000\u003c/td\u003e\n      \u003ctd\u003e226201\u003c/td\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003eDorito Corn Chp     Supreme 380g\u003c/td\u003e\n      \u003ctd\u003e200\u003c/td\u003e\n      \u003ctd\u003e650.0\u003c/td\u003e\n      \u003ctd\u003e380\u003c/td\u003e\n      \u003ctd\u003eDoritos\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e69763\u003c/th\u003e\n      \u003ctd\u003e2019-05-20\u003c/td\u003e\n      \u003ctd\u003e226\u003c/td\u003e\n      \u003ctd\u003e226000\u003c/td\u003e\n      \u003ctd\u003e226210\u003c/td\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003eDorito Corn Chp     Supreme 380g\u003c/td\u003e\n      \u003ctd\u003e200\u003c/td\u003e\n      \u003ctd\u003e650.0\u003c/td\u003e\n      \u003ctd\u003e380\u003c/td\u003e\n      \u003ctd\u003eDoritos\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"most-popular-brands\"\u003eMost popular brands\u003c/h2\u003e\n\u003cp\u003eMost popular brands overall are Kettle, Smiths, Doritos and Pringles in total appearing in 50% of transactions.\n\n  \n  \u003cscript src=\"https://cdn.plot.ly/plotly-latest.min.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cdiv id=\"/images/plotly/most_popular_brands.json\" class=\"plotly\" style=\"height:500px\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nPlotly.d3.json(\"/images/plotly/most_popular_brands.json\", function(err, fig) {\n    Plotly.plot('\\/images\\/plotly\\/most_popular_brands.json', fig.data, fig.layout, {responsive: true});\n});\n\u003c/script\u003e\u003c/p\u003e\n\u003ch3 id=\"combine-transaction-and-customer-data\"\u003eCombine transaction and customer data\u003c/h3\u003e\n\u003cp\u003eWe combined customer and transaction data by loyalty card number to understand from which customers segments our transactions are from.\u003cbr\u003e\nWe now have a datatable with 249,668 records and 12 columns.\u003c/p\u003e\n\u003ch6 id=\"now-that-we-are-done-with-preprocessing-our-dataset-and-doing-some-high-level-analysis-lets-move-on-to-exploring-the-sales-and-customer-segments-more-closely\"\u003eNow that we are done with preprocessing our dataset and doing some high level analysis, let\u0026rsquo;s move on to exploring the sales and customer segments more closely.\u003c/h6\u003e\n\u003ch1 id=\"exploring-sales\"\u003eExploring sales\u003c/h1\u003e\n\u003ch3 id=\"data-analysis-and-customer-segments\"\u003eData analysis and customer segments\u003c/h3\u003e\n\u003cp\u003eDefining metrics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003etotal sales\u003c/li\u003e\n\u003cli\u003edrivers of sales\u003c/li\u003e\n\u003cli\u003ewhere the highest sales are coming from\u003c/li\u003e\n\u003cli\u003echips bought per customer by segment\u003c/li\u003e\n\u003cli\u003eaverage chip price by customer segment\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"number-of-customers-in-each-segment\"\u003eNumber of customers in each segment\u003c/h2\u003e\n\u003cp\u003eWe have 71,517 unique customers in total.\u003c/p\u003e\n\u003cp\u003e\n\n\n\u003cdiv id=\"/images/plotly/customers_in_segments.json\" class=\"plotly\" style=\"height:500px\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nPlotly.d3.json(\"/images/plotly/customers_in_segments.json\", function(err, fig) {\n    Plotly.plot('\\/images\\/plotly\\/customers_in_segments.json', fig.data, fig.layout, {responsive: true});\n});\n\u003c/script\u003e\u003c/p\u003e\n\u003cp\u003ePremiums in lifestages:\u003cbr\u003e\nProminently budget groups are older families, young families and new families.\u003cbr\u003e\nProminently mainstream groups are young singles/couples, retirees and midage singles/couples.\u003cbr\u003e\nMost premium customers can be found in older singles/couples and retirees.\u003c/p\u003e\n\u003ch2 id=\"number-of-transactions-in-each-segment\"\u003eNumber of transactions in each segment\u003c/h2\u003e\n\u003cp\u003e\n\n\n\u003cdiv id=\"/images/plotly/transactions_in_segments.json\" class=\"plotly\" style=\"height:500px\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nPlotly.d3.json(\"/images/plotly/transactions_in_segments.json\", function(err, fig) {\n    Plotly.plot('\\/images\\/plotly\\/transactions_in_segments.json', fig.data, fig.layout, {responsive: true});\n});\n\u003c/script\u003e\u003c/p\u003e\n\u003cp\u003eMost transactions are from budget older families, mainstream retirees and mainstream young singles/couples.\u003c/p\u003e\n\u003cp\u003eLifestage:\u003cbr\u003e\n21% of all transactions are from customers in older singles/couples lifestage.\u003cbr\u003e\n19% from retirees.\u003cbr\u003e\n18% from older families.\u003cbr\u003e\n16% from younger families.\u003cbr\u003e\n14% from young singles/couples.\u003cbr\u003e\n9% from midage singles/couples.\u003cbr\u003e\nand 3% from new families.\u003cbr\u003e\nIn total that means 58% of all transactions are from older people.\u003c/p\u003e\n\u003ch2 id=\"sales-in-each-segment\"\u003eSales in each segment\u003c/h2\u003e\n\u003cp\u003e\n\n\n\u003cdiv id=\"/images/plotly/sales_in_segments.json\" class=\"plotly\" style=\"height:500px\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nPlotly.d3.json(\"/images/plotly/sales_in_segments.json\", function(err, fig) {\n    Plotly.plot('\\/images\\/plotly\\/sales_in_segments.json', fig.data, fig.layout, {responsive: true});\n});\n\u003c/script\u003e\u003c/p\u003e\n\u003cp\u003eTop sales are coming from:\u003cbr\u003e\nbudget - older families\u003cbr\u003e\nmainstream - young singles/couples\u003cbr\u003e\nmainstream - retirees\u003c/p\u003e\n\u003cp\u003eHigh sales from mainstream young singles/couples and mainstream retirees make sense because these are top number of customers in segments. 11% (top #1) of all customers are in mainstream young singles/couples and 9% (top #2) are mainstream retirees.\u003c/p\u003e\n\u003cp\u003eBudget older families though are only 6% of all customer segment, so their sales are purely driven by how often and how much they buy.\u003c/p\u003e\n\u003ch2 id=\"average-number-of-chips-bought-per-customer\"\u003eAverage number of chips bought per customer\u003c/h2\u003e\n\u003cp\u003e\n\n\n\u003cdiv id=\"/images/plotly/average_units.json\" class=\"plotly\" style=\"height:500px\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nPlotly.d3.json(\"/images/plotly/average_units.json\", function(err, fig) {\n    Plotly.plot('\\/images\\/plotly\\/average_units.json', fig.data, fig.layout, {responsive: true});\n});\n\u003c/script\u003e\u003c/p\u003e\n\u003cp\u003eOlder families and young families on average buy the most units of chips per person.(over the course of a year)\u003c/p\u003e\n\u003ch1 id=\"average-chip-price-per-segment\"\u003eAverage chip price per segment\u003c/h1\u003e\n\u003cp\u003e\n\n\n\u003cdiv id=\"/images/plotly/average_price.json\" class=\"plotly\" style=\"height:500px\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nPlotly.d3.json(\"/images/plotly/average_price.json\", function(err, fig) {\n    Plotly.plot('\\/images\\/plotly\\/average_price.json', fig.data, fig.layout, {responsive: true});\n});\n\u003c/script\u003e\u003c/p\u003e\n\u003cp\u003eAverage chip prices vary from min 3.67 to 4.06.\u003cbr\u003e\nIn mainstream midage singles/couples and mainstream young singles/couples it seems like customers are choosing chips with  slightly higher prices.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s find out if that difference is significant.\u003c/p\u003e\n\u003c!-- \u0026nbsp;   --\u003e\n\u003ch2 id=\"statistical-significance-in-average-chip-price\"\u003eStatistical significance in average chip price\u003c/h2\u003e\n\u003cp\u003eOur null hypothesis is that the average price of chips between groups is not significantly different.\u003cbr\u003e\nWe will compare average pack price in midage and young singles/couples mainstream vs budget and premium.\u003cbr\u003e\nIf the resulting p-value is smaller than 0.05 we will reject our null hypothesis and can say there is a significant difference in the average chip price for these groups.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003escipy\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003estats\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ettest_ind(mainstream, not_mainstream, equal_var\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\tTtest_indResult(statistic\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e6.72690465676574\u003c/span\u003e, pvalue\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.0340566024200332\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBecause our p-value is smaller than 0.05 we can say the higher average chip price in given mainstream groups is significant and did not happen by chance.\u003c/p\u003e\n\u003ch2 id=\"what-is-special-about-mainstream-young-singlescouples\"\u003eWhat is special about mainstream young singles/couples?\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s see why this group\u0026rsquo;s average chip price is higher than in budget and premium.\u003c/p\u003e\n\u003cp\u003eEvery other customer segment had Smiths and Kettle in their top 2 favourite chip brands. Except mainstream young singles/couples who had Doritos brand instead of Smiths.\u003c/p\u003e\n\u003ch3 id=\"how-likely-are-mainstream-young-singlescouples-to-buy-certain-brands\"\u003eHow likely are mainstream young singles/couples to buy certain brands?\u003c/h3\u003e\n\u003cp\u003eWe compared percentages of brands bought in our target group to percentages of brands bought in rest of the customers and compared the difference between those per brand.\u003c/p\u003e\n\u003cp\u003eBiggest differences in brands:\u003cbr\u003e\nMainstream young singles/couples are 23% more likely to purchase Tyrrells than others.\u003cbr\u003e\nMainstream young singles/couples are 52% less likely to purchase Woolworths chips.\u003c/p\u003e\n\u003ch3 id=\"what-about-mainstream-midage-singlescouples\"\u003eWhat about mainstream midage singles/couples?\u003c/h3\u003e\n\u003cp\u003eBiggest differences in brands:\u003cbr\u003e\nMainstream midage singles/couples are 16% more likely to purchase Kettle than others.\u003cbr\u003e\nMainstream midage singles/couples are 43% less likely to purchase Sunbites chips.\u003c/p\u003e\n\u003ch3 id=\"pack-sizes\"\u003ePack sizes\u003c/h3\u003e\n\u003cp\u003eMainstream midage and young singles/couples have a slightly higher mean in pack sizes bought compared to their premium and budget groups, but median is 170g between all.\u003cbr\u003e\n \u003c/p\u003e\n\u003ch1 id=\"conclusion\"\u003eConclusion\u003c/h1\u003e\n\u003cp\u003eTop sales are coming from:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBudget - older families\u003c/li\u003e\n\u003cli\u003eMainstream - young singles/couples\u003c/li\u003e\n\u003cli\u003eMainstream - retirees\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHigh sales from mainstream young singles/couples and mainstream retirees make sense because these are top number of customers in segments. 11% (top #1) of all customers are in mainstream young singles/couples and 9% (top #2) are mainstream retirees.\u003c/p\u003e\n\u003cp\u003eBudget older families though are only 6% of all customers, so their sales are purely driven by how often and how much they buy.\u003c/p\u003e\n\u003cp\u003eMost units of chips on average per person are bought by older families and young families.\u003c/p\u003e\n\u003cp\u003eAverage chip prices vary from 3.67 to 4.06. \u003cbr\u003e\nMainstream midage singles/couples and mainstream young singles/couples customers are buying chips with slightly higher prices than others.\u003c/p\u003e\n\u003cp\u003eWe looked into those groups more closely and found some preferences in brands.\u003cbr\u003e\nMainstream young singles/couples are:\u003cbr\u003e\n23% more likely to purchase Tyrrells than others.\u003cbr\u003e\n52% less likely to purchase Woolworths chips.\u003c/p\u003e\n\u003cp\u003eMainstream midage singles/couples are:\u003cbr\u003e\n16% more likely to purchase Kettle than others.\u003cbr\u003e\n43% less likely to purchase Sunbites chips.\u003c/p\u003e\n\u003cp\u003eBecause these groups are willing to pay more per pack of chips and have preferences with certain brands, we could recommend raising visibility of Tyrrells and Kettle chips by putting them in areas where everyone goes through, like the entrance and checkout.\u003c/p\u003e","description":null,"image":"https://res.cloudinary.com/dvhcke0re/image/upload/v1612377221/chips1-gr-stocks-OTjz3Lr9zqE-unsplash_wtslky.jpg","permalink":"http://localhost:1313/blogs/06_quantium_virtual_experience/","title":"Quantium Virtual Internship"},{"content":"\u003cp\u003eThis is a post about a virtual work experience program I completed with \u003ca href=\"https://www.theforage.com/virtual-internships/prototype/ZLJCsrpkHo9pZBJNY/ANZ-Virtual-Internship\"\u003eForage\u003c/a\u003e in their collaboration with ANZ, an Australian multinational banking and financial services company.\u003c/p\u003e\n\u003ch3 id=\"overview\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThe virtual work experiences contain a series of resources and tasks designed to simulate the real-world experience.\nThis task is based on a synthesised transaction dataset containing 3 months’ worth of transactions for 100 hypothetical customers. It contains purchases, recurring transactions, and salary transactions.\u003c/p\u003e\n\u003cp\u003eThe dataset is designed to simulate realistic transaction behaviours that are observed in ANZ’s real transaction data, so many of the insights you can gather from the tasks below will be genuine.\u003c/p\u003e\n\u003cp\u003eThis task will involve exploratory data analysis, feature engineering and predicting customers\u0026rsquo; annual salary based on their transactions.\u003c/p\u003e\n\u003cp\u003eHave a look at this \u003ca href=\"https://public.tableau.com/profile/helena.hook#!/vizhome/Transactiondatainsights/Dashboard\"\u003einteractive data dashboard\u003c/a\u003e too. And if you would like to deep dive into the code, here is a link to the \u003ca href=\"https://github.com/HHook/ANZ-virtual-internship\"\u003egithub repository\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"exploratory-data-analysis\"\u003eExploratory Data Analysis\u003c/h3\u003e\n\u003cp\u003eThe dataset given has 12 043 records with 23 columns.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt;\nRangeIndex: 12043 entries, 0 to 12042\nData columns (total 23 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   status             12043 non-null  object        \n 1   card_present_flag  7717 non-null   float64       \n 2   bpay_biller_code   885 non-null    object        \n 3   account            12043 non-null  object        \n 4   currency           12043 non-null  object        \n 5   long_lat           12043 non-null  object        \n 6   txn_description    12043 non-null  object        \n 7   merchant_id        7717 non-null   object        \n 8   merchant_code      883 non-null    float64       \n 9   first_name         12043 non-null  object        \n 10  balance            12043 non-null  float64       \n 11  date               12043 non-null  datetime64[ns]\n 12  gender             12043 non-null  object        \n 13  age                12043 non-null  int64         \n 14  merchant_suburb    7717 non-null   object        \n 15  merchant_state     7717 non-null   object        \n 16  extraction         12043 non-null  object        \n 17  amount             12043 non-null  float64       \n 18  transaction_id     12043 non-null  object        \n 19  country            12043 non-null  object        \n 20  customer_id        12043 non-null  object        \n 21  merchant_long_lat  7717 non-null   object        \n 22  movement           12043 non-null  object        \ndtypes: datetime64[ns](1), float64(4), int64(1), object(17)\nmemory usage: 2.1+ MB\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e75% of customers are between age 18 and 38, median age 28.\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/age%20dist%20V.jpg\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/age%20dist%20V.jpg\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/p\u003e\n\u003cp\u003eDifference in average salary by gender\u003cbr\u003e\nMales: 2077.94 AUD, Females: 1679.37 AUD.\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/avg%20salary%20by%20gender%20V.jpg\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/avg%20salary%20by%20gender%20V.jpg\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/p\u003e\n\u003cp\u003eAverage purchase transaction \u003cem\u003e(POS transactions)\u003c/em\u003e amount is 39.8 AUD and every customer makes 27 of them on average every month.\u003c/p\u003e\n\u003cp\u003eAverage week purchase volume and amount.\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/avg%20trans%20by%20week%20V.jpg\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/avg%20trans%20by%20week%20V.jpg\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/p\u003e\n\u003cp\u003eAverage day purchase volume and amount.\u003cbr\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/avg%20trans%20by%20hour%20V.jpg\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/avg%20trans%20by%20hour%20V.jpg\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n   \u003c/p\u003e\n\u003ch3 id=\"feature-engineering\"\u003eFeature engineering\u003c/h3\u003e\n\u003cp\u003eTo understand our customer better, we will construct some new features that relate to their purchasing behaviours.\u003c/p\u003e\n\u003cp\u003eCreate features like which state does the customer live in, how many purchases they make weekly, what is their average purchase amount, how many big purchases they make and what is their average balance.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecustomer_state \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eavg_weekly_purch_num \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eavg_weekly_trans_num \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eno_trans_days \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eavg_trans_amount \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emax_amount \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enum_large_trans \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eavg_trans_amount_overall \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003emed_balance \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;customer_id\u0026#39;\u003c/span\u003e]:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    customer_state\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(customer_state_df[customer_state_df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;id\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003ei][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;state\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eitem())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    avg_weekly_purch_num\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(int(weekly_purch\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eloc[i]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    avg_weekly_trans_num\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(int(weekly_trans\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eloc[i]))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    no_trans_days\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(pos_df[pos_df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;customer_id\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003ei][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;date\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003enunique())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    avg_trans_amount\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(int(round(pos_df[pos_df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;customer_id\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003ei][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;amount\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emean(), \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    max_amount\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(round(pos_df[pos_df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;customer_id\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003ei][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;amount\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emax(), \u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    num_large_trans\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(pos_df[(pos_df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;customer_id\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003ei) \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003e (pos_df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;amount\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e\u0026gt;\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e100\u003c/span\u003e)][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;amount\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecount())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    avg_trans_amount_overall\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(int(round(df[df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;customer_id\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003ei][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;amount\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emean(), \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e)))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    med_balance\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eappend(df[df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;customer_id\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003ei][\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;balance\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emedian())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;state\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e customer_state\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;avg_weekly_purch_num\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e avg_weekly_purch_num\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;avg_weekly_trans_num\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e avg_weekly_trans_num\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;no_trans_days\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e no_trans_days\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;avg_trans_amount\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e avg_trans_amount\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;max_amount\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e max_amount\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;num_large_trans\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e num_large_trans\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;avg_trans_amount_overall\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e avg_trans_amount_overall\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;med_balance\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e med_balance\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e \u003c/p\u003e\n\u003ch5 id=\"age-bins\"\u003eAge bins\u003c/h5\u003e\n\u003cp\u003eWe will also bin the customer age to one four categories: below 20, between 20 and 40, between 40 and 60, over 60.\u003c/p\u003e\n\u003cp\u003e \u003c/p\u003e\n\u003ch5 id=\"annual-salary-for-each-customer\"\u003eAnnual salary for each customer\u003c/h5\u003e\n\u003cp\u003eFind only salary payments.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edf_salaries \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e df[df[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;txn_description\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;PAY/SALARY\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003egroupby(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;customer_id\u0026#39;\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esum()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eEvery customer in the dataset has a different frequency for receiving their salary, some get paid once a month, some every week. To find an average monthly salary we will sum up all their payments in the three months of data we have and divide it by three.  To then get the annual salary we multiply the average monthly salary with 12.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eannual_salaries \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e df_salaries[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;amount\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e/\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e12\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e# annual salary for each customer\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/annual_salary_dist_V.jpg\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/annual_salary_dist_V.jpg\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\n\u003cp\u003e \u003c/p\u003e\n\u003ch2 id=\"predicting-annual-salary\"\u003ePredicting annual salary\u003c/h2\u003e\n\u003cp\u003eFirst we will need to do some dummy encoding on categorical variables like \u0026lsquo;status\u0026rsquo;, \u0026rsquo;txn_description\u0026rsquo;, \u0026lsquo;gender\u0026rsquo;, \u0026lsquo;state\u0026rsquo; and \u0026lsquo;age_bin\u0026rsquo;.\u003c/p\u003e\n\u003cp\u003eThen check correlation between variabels and \u0026lsquo;annual_salary\u0026rsquo;.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003enum_feats\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003ecorr()[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;annual_salary\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esort_values(ascending\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cpre\u003e\u003ccode\u003eannual_salary           1.000000\navg_trans_amount_ov     0.538656\nmed_balance             0.258076\nbalance                 0.257159\namount                  0.091111\navg_trans_amount        0.044312\nnum_large_trans        -0.045275\navg_weekly_trans_num   -0.079352\nmax_amount             -0.097739\nno_trans_days          -0.172765\navg_weekly_purch_num   -0.189532\nName: annual_salary, dtype: float64\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere I realised avg_trans_amount_overall is suspiciously highly correlated. I decided to not use this as a feature for our predictive model because the calculation includes salary payments and we are trying to predict customers\u0026rsquo; annual salary just by their purchasing behaviour, without knowing about their salary payments.\u003c/p\u003e\n\u003cp\u003eFor testing, I chose to split our data to 70% train and 30% for test set.\u003c/p\u003e\n\u003ch3 id=\"linear-regression\"\u003eLinear regression\u003c/h3\u003e\n\u003cp\u003eLet\u0026rsquo;s build a simple linear regression  model with sklearn to predict customers\u0026rsquo; annual salary with features we defined earlier.\u003c/p\u003e\n\u003cp\u003eOur linear model valuation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eR-squared 0.5381890117545585\nMAE: 13706.815968071387\nMSE: 323705668.54002833\nRMSE: 17991.822268464868\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePlotting the predictions. Our model\u0026rsquo;s predictions are plotted in blue, perfect predictions in red.\u003cbr\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/output_39_1.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/output_39_1.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/p\u003e\n\u003cp\u003eOur model\u0026rsquo;s predictions are kind of following the trend, but still too scattered to be trusted. So our linear regression model did not perform the best and this would not be an accurate way to predict customers\u0026rsquo; annual salaries. But let\u0026rsquo;s try using a different model.\u003c/p\u003e\n\u003ch3 id=\"gradient-boosting\"\u003eGradient boosting\u003c/h3\u003e\n\u003cp\u003eGradient boosting is an ensemble learner. This means it will create a final model based on a collection of individual decision tree models.\u003c/p\u003e\n\u003cp\u003eOur gradient boosting model performed noticably better than the linear regression earlier:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eR-squared 0.9842118273179078\nMAE: 2476.2601909654327\nMSE: 11066694.217258077\nRMSE: 3326.66412750943\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePlotting the predictions. Our gradient boosting model’s predictions are plotted in blue, perfect predictions in red.\u003cbr\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/output_56_1.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/output_56_1.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/p\u003e\n\u003cp\u003eA gradient descent procedure is used to minimize the loss when adding trees. Our model\u0026rsquo;s loss is minimsed at around 400 trees.\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/output_loss.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/output_loss.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/p\u003e\n\u003ch2 id=\"results\"\u003eResults\u003c/h2\u003e\n\u003cp\u003eOur gradient boosting machine definitely outperformed the linear regression model. Should it be used to segment customers whose annual salary we do not have? Maybe not yet. We could definitely do better with more data \u003cem\u003e(real data)\u003c/em\u003e and more parameter tuning.\u003c/p\u003e","description":null,"image":null,"permalink":"http://localhost:1313/blogs/05_anz_virtual_experience/","title":"ANZ Virtual Internship"},{"content":"\u003cp\u003eThis is a solution I proposed for a data based problem while working in a successful five star luxury hotel cocktail bar.\u003c/p\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eAs a supervisor in one of London’s award winning cocktail bars, where expectations are high and people come to visit from all over the world: I was responsible for overseeing the everyday workflow in the bar: supporting my team, looking after the guests and making sure our standards are being followed consistently.\u003c/p\u003e\n\u003cp\u003eThe selection of spirits available to sell in our bar ranged from 10-50 different products per category with prices from ££-££££ per double shot, which means it is possible for every and any guest to find something that suits their palate with a little help from the bar team, theoretically. Working alongside my team I noticed on daily basis the team would recommend the same few spirits to new and old customers. After gathering the sales data of spirits sold in the last six months, this habit showed in the variety of spirits sales or better – the lack thereof.\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe problem\u003c/h2\u003e\n\u003cp\u003eOn the graphs you can see sales quantities for three spirit categories over the last 6 months, excluding the use of spirits in cocktails.\nAfter the top 5-6 most popular spirits of each category, the sale of rest of the products drops dramatically. The green columns mark the house spirits \u003cem\u003e(which are used when no specification is made)\u003c/em\u003e and as you can see in all three of these categories the house spirits are presented in the top 5 as these are the products we mention to customers the most.\nThe price of the spirits chosen stays relatively the same in the top 3, \u003cem\u003e(with an exception of Zacapa 23yo rum which due to its general popularity managed to reach top 3)\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-33\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/gin2.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/gin2.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-33\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/vodka2.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/vodka2.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-33\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/rum2.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/rum2.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/p\u003e\n\u003cp\u003e \u003c/p\u003e\n\u003cp\u003eThe problem I wanted to tackle with this data was to boost teams confidence in promoting the wider of range of products available and increasing profit as the result. Knowing my team I was aware of the inconsistent knowledge base between team members, some knew more some less. I was sure the main reason behind the lack of variety in sales was the unfamiliarity with products available. Being unfamiliar with what you are presenting and selling makes you miss the opportunity of enhancing the guest experience by more engagement.\u003c/p\u003e\n\u003ch2 id=\"the-goal\"\u003eThe goal\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eMotivate team members, encourage them to learn new things, learn about themselves and share their knowledge\u003c/li\u003e\n\u003cli\u003eEnhance guest experience by more engagement and explaining of products\u003c/li\u003e\n\u003cli\u003eIncrease profit on spirit sales as a result of engagement with customers\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis can only be done by knowing well the products we have on offer, which brings me to\u0026hellip;.\u003c/p\u003e\n\u003ch2 id=\"proposed-solution\"\u003eProposed solution\u003c/h2\u003e\n\u003cp\u003eI chose as a solution for this problem to give each team member an individual project to work on. This would make them learn deeply one particular subject, for example rum/gin/vodka and share the information learned with the team. Essentially giving their own little masterclass to the team. They would need to explain specific products sold at the bar and what makes them different from each other.\u003cbr\u003e\nThis would encourage them to take ownership for their project and know that their efforts put into this would affect the knowledge of the whole team. The sharing part is crucial, as at that point team members need to put what they have learned in their own words and make it clear for the rest of the team. This is a team effort with great benefits for everyone.\u003c/p\u003e\n\u003ch2 id=\"how-new-knowledge-would-translate-at-work\"\u003eHow new knowledge would translate at work\u003c/h2\u003e\n\u003cp\u003eUpselling is not about selling more, it is about opportunity. The opportunity to deliver a great service experience, to truly exceed a customer’s expectations and profits as a result. This would allow the customer to try something new, broaden their knowledge and encourages return business to the bar.\u003c/p\u003e","description":null,"image":null,"permalink":"http://localhost:1313/blogs/04_increasing_profit_in_spirits_sales/","title":"Increasing profit in a five star luxury environment"},{"content":"\u003ch4 id=\"continuation-of-udacity-project\"\u003eContinuation of Udacity project\u003c/h4\u003e\n\u003cp\u003eThis time we will go through this project with Python using scikit-learn and see how the results differ from eachother from previous iteration.\u003cbr\u003e\nWe’re tasked with predicting how much money a fictional company in the mail-order catalog business can expect to earn from sending out a catalog to new customers.\u003c/p\u003e\n\u003ch3 id=\"overview\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eThis project has previously been finished using Alteryx analytics software as \u003ca href=\"https://hhook.github.io/2020/04/predicting-catalog-demand/\"\u003eproject work\u003c/a\u003e for Udacity\u0026rsquo;s Predictive Analytics for Business nanodegree.\u003c/p\u003e\n\u003cp\u003eThis task will involve building a linear regression model with scikit-learn and applying the results in order to provide a recommendation to this fictional management.\u003c/p\u003e\n\u003cp\u003eWe have two excel files to work with:\u003cbr\u003e\n1. customers.xlsx - existing customers\u003cbr\u003e\n2. mailinglist.xlsx - new customers\u003c/p\u003e\n\u003ch3 id=\"the-business-problem\"\u003eThe Business Problem\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eLast year the company sent out its first print catalog, and is preparing to send out this year’s catalog in the coming months. The company has 250 new customers from their mailing list that they want to send the catalog to. Determine how much profit the company can expect from sending a catalog to these customers. Management does not want to send the catalog out to these new customers unless the expected profit contribution exceeds $10,000.\u003c/em\u003e\u003c/p\u003e\n\u003ch4 id=\"business-and-data-understanding\"\u003eBusiness and Data Understanding\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003eWe need to predict if this catalog campaign is a good idea. Should the company send out these catalogs to new customers? How much revenue can this company expect to make if they send their new catalog to these 250 customers? Is it profitable considering all the costs and possibility that customer might not make a purchase at all?\u003c/li\u003e\n\u003cli\u003eWhat data is needed to inform those decisions?\n\u003cul\u003e\n\u003cli\u003ePossible revenue from each client\u003c/li\u003e\n\u003cli\u003eCost of each catalog\u003c/li\u003e\n\u003cli\u003eAverage past sales\u003c/li\u003e\n\u003cli\u003eAverage gross margin for each product\u003c/li\u003e\n\u003cli\u003eAverage number of products purchased\u003c/li\u003e\n\u003cli\u003eProbability that customer will buy\u003c/li\u003e\n\u003cli\u003eCustomer segment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"eda\"\u003eEDA\u003c/h3\u003e\n\u003cp\u003eIn the exploratory data analysis part we were able to see that our existing customer data on response to the last catalog is imbalanced towards \u0026lsquo;No\u0026rsquo;.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecustomers[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Responded_to_Last_Catalog\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003evalue_counts()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cpre\u003e\u003ccode\u003eNo     2204\nYes     171\nName: Responded_to_Last_Catalog, dtype: int64\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBecause we are working with fake data, we already have the new customers probable response to our new catalog in our data, with a probability between 0 and 1. If we did not have this we would have to calculate the probability with a logistic regression model and training the model with such an imbalance(13:1) would be problematic.\u003c/p\u003e\n\u003ch3 id=\"choosing-features\"\u003eChoosing features\u003c/h3\u003e\n\u003cp\u003eFor our linear regrssion features we have kept Customer_Segment, Avg_Sale_Amount and Avg_Num_Products_Purchased. Because we want to use Customer_Segment in our model as well, but is a categorical variable, we need to convert it to dummy variables.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efeats[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Customer_Segment\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003evalue_counts()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cpre\u003e\u003ccode\u003eStore Mailing List              1108\nLoyalty Club Only                579\nCredit Card Only                 494\nLoyalty Club and Credit Card     194\nName: Customer_Segment, dtype: int64\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003eConvert the 4 categorical values to dummy variables:\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eseg_dummies \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eget_dummies(feats[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Customer_Segment\u0026#39;\u003c/span\u003e], prefix\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Seg_\u0026#39;\u003c/span\u003e, drop_first\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efeats \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e pd\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003econcat([feats, seg_dummies], axis\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efeats \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e feats\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edrop(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Customer_Segment\u0026#39;\u003c/span\u003e, axis\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e1\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"linear-regression\"\u003eLinear regression\u003c/h3\u003e\n\u003cp\u003eWe will use Avg_Sale_Amount from old customers\u0026rsquo; data as the target variable to predict the average sale amount for new customers.\u003cbr\u003e\nAfter splitting our data for testing and training the \u003cstrong\u003eBest linear regression equation\u003c/strong\u003e was the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eY = 284.48 + 70.68 * Avg_Num_Products_Purchased - 144.33 * (If Type: Loyalty Club Only) + 267.64 * (If Type: Loyalty Club and Credit Card) - 231.33 * (If Type: Store Mailing List) + 0 * (If Type: Credit Card Only) \n\u003c/code\u003e\u003c/pre\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/output_41_1.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/output_41_1.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\n\u003cpre\u003e\u003ccode\u003eR-squared 0.8485331100796489\nMAE: 89.31308194968375\nMSE: 17006.43868059899\nRMSE: 130.40873697954055\n\u003c/code\u003e\u003c/pre\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003efeats[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Avg_Sale_Amount\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003edescribe()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    count    \u003cspan style=\"color:#ae81ff\"\u003e2375.000000\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    mean      \u003cspan style=\"color:#ae81ff\"\u003e399.774093\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    std       \u003cspan style=\"color:#ae81ff\"\u003e340.115808\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    min         \u003cspan style=\"color:#ae81ff\"\u003e1.220000\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ae81ff\"\u003e25\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e168.925000\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ae81ff\"\u003e50\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e281.320000\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ae81ff\"\u003e75\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e%\u003c/span\u003e       \u003cspan style=\"color:#ae81ff\"\u003e572.400000\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    max      \u003cspan style=\"color:#ae81ff\"\u003e2963.490000\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    Name: Avg_Sale_Amount, dtype: float64\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"average-sales-for-new-customer\"\u003eAverage sales for new customer\u003c/h4\u003e\n\u003cp\u003eNow we are ready to deploy our linear regression model on new customers data and then multiply the predicted revenue with given customer’s probability of making the purchase:\u003cbr\u003e\n\u003cem\u003eProbable_Revenue = (Predicted_Avg_Sale_Amount) * (Answer_Yes)\u003c/em\u003e\u003c/p\u003e\n\u003ch3 id=\"results\"\u003eResults\u003c/h3\u003e\n\u003cp\u003eDoes the expected profit contribution exceed $10,000?\u003c/p\u003e\n\u003cp\u003eProbable revenue is multiplied with average gross margin of all products, which is 50% and deducted the cost of each catalog which is $6.50:\u003cbr\u003e\n\u003cem\u003eProbable_Revenue * 0.5 – 6.50\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprob_revenue[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Rev_Minus_cost\u0026#39;\u003c/span\u003e] \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e (prob_revenue[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Prob_Revenue\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e)\u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e6.50\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eprob_revenue[\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;Rev_Minus_cost\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esum()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cpre\u003e\u003ccode\u003e22012.86333703793\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe Final Profit from the new catalog is expected to be \u003cstrong\u003e$22,012.86\u003c/strong\u003e which would be the probable profit made from sending new catalogs out to given 250 customers.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u0026mdash;Notes\u0026mdash;\u003c/em\u003e\u003cbr\u003e\nThe results compared with previous rendering of linear regression model in Alteryx software for the same data is almost the same.\u003cbr\u003e\nTheir R-squared values were slightly different, where Alteryx model calculated 0.8369 and our model in sckit-learn 0.8485.\u003cbr\u003e\nSo Alteryx predicted the final profit to be $21,987.44, which results in a $25.42 difference.\u003c/p\u003e","description":null,"image":null,"permalink":"http://localhost:1313/blogs/03_predicting_catalog_deman_sklearn/","title":"Predicting Catalog Demand - with scikit-learn"},{"content":"\u003cp\u003eIn this project, we need to decide if a new bank customer is approved for loan or not using a predictive model.\u003c/p\u003e\n\u003ch5 id=\"project-overview\"\u003eProject overview\u003c/h5\u003e\n\u003cp\u003eWe have the following information to work with:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eData on all past applications for a loan\u003c/li\u003e\n\u003cli\u003eThe list of customers that need to be classified\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBecause we are classifying new customers to two groups: creditworthy or not - we need to use a binary model to make this decision. We will build and compare classification models such as Logistic Regression, Decision Tree, Random Forest, and Boosted Model models’ performance against each other. Then decide on the best model to classify new bank customers on whether they can be approved for a loan or not.\u003c/p\u003e\n\u003ch2 id=\"step-1-data-clean-up-and-building-the-training-set\"\u003eStep 1: Data clean up and building the Training Set\u003c/h2\u003e\n\u003cp\u003eI used Field Summary tool to explore given dataset in Alteryx. Based on report below following decisions were made:\nDuration in Current Address has 69% missing data and was removed, while Age Years has missing data as well, but only 2.4% which I decided to impute the missing values with median age because median is not affected by outliers.\nThere were two fields with just one value for the whole field: Occupation and Concurrent Credits. Other fields with low-variability where more than 80% of the data was skewed to one side: Guarantors, Number of Dependants and Foreign Worker. All these fields were removed.\nAnd lastly Telephone field was removed because it is irrelevant to customer’s credit worth.\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-50\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/Predictors%20removed.JPG\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/Predictors%20removed.JPG\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-50\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/Occupation%20uniform.JPG\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/Occupation%20uniform.JPG\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n   \u003c/p\u003e\n\u003ch2 id=\"step-2-training-the-classification-models\"\u003eStep 2: Training the Classification Models\u003c/h2\u003e\n\u003cp\u003eFor creating estimation and validation samples 70% went for estimation(training sample) and 30% of the entire dataset was saved for validation.\nFor each model I have highlighted their significant predictor variables with their p-values.   \u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLogistic Regression model\nLogistic regression model with Stepwise kept these predictor variables:\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-33 right\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/LR%20model%20predictor%20p%20values.JPG\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/LR%20model%20predictor%20p%20values.JPG\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eAccount balance\u003c/li\u003e\n\u003cli\u003ePayment status\u003c/li\u003e\n\u003cli\u003ePurpose\u003c/li\u003e\n\u003cli\u003eCredit amount\u003c/li\u003e\n\u003cli\u003eLength of current employment\u003c/li\u003e\n\u003cli\u003eInstalment percent  \u003cbr\u003e\n \u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eDecision Tree model\nDecision Tree model found these predictor variables significant:\u003c/li\u003e\n\u003c/ol\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-33 right\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/DT%20model%20predictors%20VI.JPG\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/DT%20model%20predictors%20VI.JPG\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\n\u003cul\u003e\n\u003cli\u003eAccount Balance\u003c/li\u003e\n\u003cli\u003eValue Savings Stocks\u003c/li\u003e\n\u003cli\u003eDuration of Credit in Months  \u003cbr\u003e\n \u003cbr\u003e\n \u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eForest Model\nThe Forest model found these predictor variables most significant:\u003cbr\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-25 right\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/F%20model%20predictors%20VI.JPG\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/F%20model%20predictors%20VI.JPG\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eCredit Amount\u003c/li\u003e\n\u003cli\u003eAge in years\u003c/li\u003e\n\u003cli\u003eDuration of credit in months\u003c/li\u003e\n\u003cli\u003eAccount Balance  \u003cbr\u003e\n \u003cbr\u003e\n \u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eBoosted Model\nOur Boosted Model works with these predictor variables:\u003cbr\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-25 right\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/B%20model%20predictors%20VI.JPG\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/B%20model%20predictors%20VI.JPG\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eCredit amount\u003c/li\u003e\n\u003cli\u003eAccount balance\u003c/li\u003e\n\u003cli\u003eDuration of credit in months\u003c/li\u003e\n\u003cli\u003ePayment status of previous credit  \u003cbr\u003e\n \u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"step-3-model-accuracy-comparison-against-validation-set\"\u003eStep 3: Model accuracy comparison against validation set\u003c/h2\u003e\n\u003cp\u003e\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\n\u003cdiv class=\"figure center fig-75\" \u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/Comparison%20report.JPG\" \u003e\n  \n  \n\u003c/div\u003e\n\n  \u003cdiv style=\"clear:both;\"\u003e\u003c/div\u003e\n\nModel accuracy comparison report tells us the overall accuracy for Logistic Regression model was 75%, Decision Tree model 75%, Forest model 80% and Boosted model 78%.\n \u003cbr\u003e\n \u003cbr\u003e\n\n \n  \n  \n  \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n  \n\n\n\u003cdiv class=\"figure nocaption fig-75 center\" \u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/Comparison%20report%20confusion%20matrix.JPG\" \u003e\n  \n  \n\u003c/div\u003e\n\n  \u003cdiv style=\"clear:both;\"\u003e\u003c/div\u003e\n\u003c/p\u003e\n\u003cp\u003eThe Regression and the Decision Tree models predicted a lot of creditworthy applicants as non-creditworthy. Those two models are predicting the creditworthy applicants with more accuracy compared to the non-creditworthy. We can see that the Positive Predictive value \u003cem\u003e(PPV, also called Precision)\u003c/em\u003e is higher than the Negative Predictive Value \u003cem\u003e(NPV)\u003c/em\u003e which suggests that the models are biased towards creditworthy applicants which is understandable since our data set is biased - having a lot more creditworthy than non-creditworthy applicants.\u003c/p\u003e\n\u003cp\u003eDecision tree and regression model both: PPV 79%, NPV 60%.\u003cbr\u003e\nBoosted model: PPV 77%, NPV 80%.\u003cbr\u003e\nForest model: PPV 80%, NPV 83%.\u003c/p\u003e\n\u003cp\u003eThe PPV and NPV are almost equal in Boosted and Forest models - suggesting lack of bias.\u003c/p\u003e\n\u003ch2 id=\"step-4-choosing-the-best-model\"\u003eStep 4: Choosing the best model\u003c/h2\u003e\n\u003cp\u003eI chose to use our Forest model to predict loan worthy customers taking into account its highest overall accuracy compared to other models and its high F1 score, which looks for the bias in prediciting \u0026ldquo;Creditworthy\u0026rdquo; or \u0026ldquo;Non-Creditworthy\u0026rdquo; and calculates a single score: the higher the score, the better. Our Forest model’s overall accuracy is 80%, F1 87%.\u003c/p\u003e\n\u003cp\u003ePPV = True Positives / ( True Positives + False Positives ) = 101/ (101 + 26) = 0.8\u003cbr\u003e\nNPV = True Negatives / ( True Negatives + False Negatives) = 19 / ( 19 + 4) = 0.83\u003cbr\u003e\nACC = (TP + TN) / (P + N) = (101 + 19) / (127 + 23) = 0.8\u003cbr\u003e\nF1 = 2TP / (2TP + FP + FN) = 2x101/ (2x101 + 26 + 4) = 0.87\u003c/p\u003e\n\u003cp\u003e\n\n\n\u003cdiv class=\"figure nocaption fig-33 right\" \u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/Comparison%20report%20ROC.JPG\" \u003e\n  \n  \n\u003c/div\u003e\n\nIn the ROC curve we see our Forest model with orange line being the highest in graph of all models – meaning that we are getting a higher rate of true positive rates vs. false positives rates. We are looking for a high rate of true positive vs. true negative rates because we do not want to extend loans to people who are not creditworthy. Classifiers that get curves closer to the top-left corner indicate a better performance.\u003c/p\u003e\n\u003cp\u003eOur Forest model’s AUC is 0.73 it means there is 73% chance that model will be able to distinguish between positive class and negative class.\u003c/p\u003e\n\u003ch3 id=\"the-result\"\u003eThe result\u003c/h3\u003e\n\u003cp\u003eOut of 500 new applications there are 411 customers who are creditworthy meaning they qualify for getting a loan as per our prediction model.\u003cbr\u003e\n\n \n  \n  \n  \n  \n    \n      \n    \n  \n    \n  \n\n\n\u003cdiv class=\"figure nocaption\" \u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/Scoring%20creditworthy%20customers.JPG\" \u003e\n  \n  \n\u003c/div\u003e\n\n  \u003cdiv style=\"clear:both;\"\u003e\u003c/div\u003e\n\u003c/p\u003e\n\u003cp\u003e \u003cbr\u003e\n \u003c/p\u003e\n\u003ch5 id=\"materials-used\"\u003eMaterials used:\u003c/h5\u003e\n\u003cp\u003e\u003cem\u003e“Understanding AUC - ROC Curve”\u003c/em\u003e\u003cbr\u003e\n\u003ca href=\"https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\"\u003ehttps://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e“Simple guide to confusion matrix terminology”\u003c/em\u003e\u003cbr\u003e\n\u003ca href=\"https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\"\u003ehttps://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e“Confusion matrix online calculator”\u003c/em\u003e\u003cbr\u003e\n\u003ca href=\"http://onlineconfusionmatrix.com/\"\u003ehttp://onlineconfusionmatrix.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e \u003cbr\u003e\nAlteryx workflows:\u003cbr\u003e\n\n \n  \n  \n  \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n  \n\n\n\u003cdiv class=\"figure  fig-75 center\" \u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/Comparison%20workflow.JPG\"  alt=\"Model comparison workflow\"\u003e\n  \n   \n    \u003cspan class=\"caption\"\u003eModel comparison workflow\u003c/span\u003e\n  \n\u003c/div\u003e\n\n  \u003cdiv style=\"clear:both;\"\u003e\u003c/div\u003e\n\n\n \n  \n  \n  \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n    \n  \n\n\n\u003cdiv class=\"figure  fig-75 center\" \u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/Scoring%20forest%20model.JPG\"  alt=\"Scoring data with Forest model  \"\u003e\n  \n   \n    \u003cspan class=\"caption\"\u003eScoring data with Forest model  \u003c/span\u003e\n  \n\u003c/div\u003e\n\n  \u003cdiv style=\"clear:both;\"\u003e\u003c/div\u003e\n\u003c/p\u003e","description":null,"image":null,"permalink":"http://localhost:1313/blogs/02_predicting_default_risk_ud/","title":"Predicting Default Risk"},{"content":"\u003cp\u003eIn this project, we will analyze a fictional business problem in the mail-order catalog business. We\u0026rsquo;re tasked with predicting how much money your company can expect to earn from sending out a catalog to new customers.\u003c/p\u003e\n\u003cp\u003eThis task will involve building a linear regression model with Alteryx and applying the results in order to provide a recommendation to management.\u003c/p\u003e\n\u003ch6 id=\"the-business-problem\"\u003eThe Business Problem\u003c/h6\u003e\n\u003cp\u003eLast year the company sent out its first print catalog, and is preparing to send out this year\u0026rsquo;s catalog in the coming months. The company has 250 new customers from their mailing list that they want to send the catalog to.\nDetermine how much profit the company can expect from sending a catalog to these customers. Management does not want to send the catalog out to these new customers unless the expected profit contribution exceeds $10,000.\u003c/p\u003e\n\u003ch2 id=\"step-1-business-and-data-understanding\"\u003eStep 1: Business and Data Understanding\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eWe need to predict if this catalog campaign is a good idea.\nShould the company send out these catalogs to new customers? How much revenue can this company expect to make if they send their new catalog to these 250 customers? Is it profitable considering all the costs and possibility that customer might not make a purchase at all?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhat data is needed to inform those decisions?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003ePossible revenue from each client.\u003c/li\u003e\n\u003cli\u003eCost of each catalog.\u003c/li\u003e\n\u003cli\u003eAverage past sales.\u003c/li\u003e\n\u003cli\u003eAverage gross margin for each product.\u003c/li\u003e\n\u003cli\u003eAverage number of products purchased.\u003c/li\u003e\n\u003cli\u003eProbability that customer will buy.\u003c/li\u003e\n\u003cli\u003eCustomer segment.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"step-2-analysis-modeling-and-validation\"\u003eStep 2: Analysis, Modeling, and Validation\u003c/h2\u003e\n\u003cp\u003eVariables for each customer given in the data: Name, Customer Segment, Customer ID, Address, Average Sale Amount, Response to Last Catalog, Average Number of Products Purchased, Years as a Customer.\u003c/p\u003e\n\u003cp\u003eOn my first model I chose to use these predictor variables: Average number of products purchased, Years as a Customer and Customer segment.\nFor Numeric variables we can use scatterplots between an individual variable and the target variable to see if a variable might be a good candidate for a predictor variable. For categorical variables we need to run them through the model to see their if they are significant. We don’t want to include any variables that aren’t statistically significant. If the p-value is less than 0.05, we can be 95% confident that there exists a relationship between the predictor and target variable.\u003c/p\u003e\n\u003cp\u003e\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption fig-50\" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/avg%20number%20of%20products%20purchased%20vs%20avg%20sale%20amount.JPG\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/avg%20number%20of%20products%20purchased%20vs%20avg%20sale%20amount.JPG\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\u003cbr\u003e\nAverage number of products purchased as a continuous variable is easy to\nsee the correlation between the predictor variable and target variable by visualizing it. I put Average number of products purchased and Average sale amount on a scatterplot to visualize this.\u003c/p\u003e\n\u003cp\u003eTo use the categorical variable of Customer segment in our model we have to create dummy variables. Because we can’t check categorical variables’ relationship visually, we run it through the model and check their p-values. Looking for p-values to be smaller than 0.05 which makes them statistically significant. Reading the regression model report we see this is true for our Customer segments.\u003c/p\u003e\n\n\n \n  \n  \n  \n  \n    \n  \n    \n      \n    \n  \n    \n      \n    \n  \n\n\u003cdiv class=\"figure nocaption \" \u003e\n  \n    \u003ca class=\"fancybox\" href=\"/images/2nd%20model%20coefficients.png\" data-fancybox-group=\"\"\u003e\n  \n    \u003cimg class=\"fig-img\" src=\"/images/2nd%20model%20coefficients.png\" \u003e\n  \n    \u003c/a\u003e\n  \n  \n\u003c/div\u003e\n\n\u003cp\u003eFrom Linear Regression Model Report we can also see the p-value of Average number of products purchased is 2.2e-16 \u003cem\u003e(this can be written as 2.2 * 10^-16 which can also be written as \u0026lt; .00000000000000022 This is very small!)\u003c/em\u003e, which makes it statistically significant.\u003c/p\u003e\n\u003cp\u003eBecause the trendline on scatterplot between Years as a Customer and Average Sale Amount came out flat \u003cem\u003e(for it being a discrete variable)\u003c/em\u003e, I decided to run it through the linear regression model to see if it has any significance. The p-value of this turned out to be 0.056 and we can consider it to be of insignificant value to the model and remove it from the equation.\u003c/p\u003e\n\u003ch6 id=\"why-i-think-it-is-a-good-model\"\u003eWhy I think it is a good model\u003c/h6\u003e\n\u003cp\u003eFirst, with all the p-values of predictors used in this model being less than 0.05 we consider all of them significant, which means the relationship is not likely to be occurring randomly.\nSecond, we need to look at the adjusted R-squared value. \u003cem\u003eAdjusted\u003c/em\u003e R-squared because it is a multiple linear regression. In our results it  is 0.84 and this is considered a very good result, generally higher is better.\nTogether the low p-values of predictor variables and R-squared value show us the model is highly predictive which suggests it to be a good model for the results we need and the available data we have.\u003c/p\u003e\n\u003cp\u003eBest linear regression equation based on the available data is the following:\u003cbr\u003e\nY = 303.46 + 66.98 * Avg_Num_Products_Purchased - 149.36 * (If Type: Loyalty Club Only) + 281.84 * (If Type: Loyalty Club and Credit Card) - 245.42 * (If Type: Store Mailing List) + 0 * (If Type: Credit Card Only)\u003c/p\u003e\n\u003ch2 id=\"step-3-presentationvisualization\"\u003eStep 3: Presentation/Visualization\u003c/h2\u003e\n\u003ch6 id=\"my-recommendation\"\u003eMy recommendation\u003c/h6\u003e\n\u003cp\u003eBased on my predictive model and calculations made considering costs, as the management wanted to send out the catalog to these 250 customers only if the expected profit exceeds $10,000 - I can recommend them to do so.\u003c/p\u003e\n\u003cp\u003eMy recommendation to send out the catalogs is based on the use of created predictive model:\nPredicted_Revenue = 303.46 + 66.98 * Avg_Num_Products_Purchased - 149.36 * (If Type: Loyalty Club Only) + 281.84 * (If Type: Loyalty Club and Credit Card) - 245.42 * (If Type: Store Mailing List) + 0 * (If Type: Credit Card Only)\u003c/p\u003e\n\u003cp\u003eThen we multiply the predicted revenue with given customer’s probability of making the purchase:\nProbable_Revenue = (Predicted_Revenue) * (Probability to buy)\u003cbr\u003e\nThis is multiplied with average gross margin of all products, which is 50% and deduct the cost of each catalog which is $6.50:\nProbable_Revenue * 0.5 – 6.50\u003c/p\u003e\n\u003cp\u003eThe Final Profit from the new catalog is expected to be $21,987.44 which would be the probable profit made from sending new catalogs out to given 250 customers.\u003c/p\u003e","description":null,"image":null,"permalink":"http://localhost:1313/blogs/01_predicting_catalog_demand_ud/","title":"Predicting Catalog Demand"}]